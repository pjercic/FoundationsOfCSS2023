{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01196ea9-5dff-4279-b0a7-d9ea286e0c19",
   "metadata": {},
   "source": [
    "# Social Network Analysis of Swiss Politicians on Twitter Data\n",
    "## Tasks\n",
    "In this assignment you will do the following tasks:\n",
    "1. Construct the timelines of Twitter users\n",
    "2. Build social network of retweets\n",
    "3. Calculate assortativity\n",
    "4. Permutation tests\n",
    "5. Community detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60aac7b3",
   "metadata": {},
   "source": [
    "### Install requirements. \n",
    "\n",
    "The following cell contains all the necessary dependencies needed for this task. If you run the cell everything will be installed.  \n",
    "\n",
    "* [`pandas`](https://pandas.pydata.org/docs/index.html) is a Python package for creating and working with tabular data. [Here](https://pandas.pydata.org/docs/reference/index.html) is the documentation of `pandas`.\n",
    "* [`numpy`](https://numpy.org/) is a Python package for mathematical functions. [Here](https://numpy.org/doc/stable/reference/index.html) is the documentation of `numpy`.\n",
    "* [`matplotlib`](https://matplotlib.org/) is a Python package for creating plots. [Here](https://matplotlib.org/stable/api/index.html) is the documentation of `matplotlib`.\n",
    "* [`networkx`](https://networkx.org/) is a Python package for the creation, manipulation, and study of the structure, dynamics, and functions of complex networks. [Here](https://networkx.org/documentation/stable/reference/index.html) is the documentation of `networkx`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234eb790-6def-45a2-b436-406eb1be62ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pandas\n",
    "! pip install numpy\n",
    "! pip install matplotlib\n",
    "! pip install networkx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebb2e59",
   "metadata": {},
   "source": [
    "### Import requirements\n",
    "The cell below imports all necessary dependancies. Make sure they are installed (see cell above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e909d35-ff4c-45fc-8642-c2be8d3850c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timezone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89432616-10db-4283-997c-90a243f41f60",
   "metadata": {},
   "source": [
    "# 1. Construct the timelines of Twitter users"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b451349f-8eec-4534-8057-6e491546c024",
   "metadata": {},
   "source": [
    "You can use the provided `users.csv` file since Twitter API is unavailable at the moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0aa70e-cbc2-4494-b088-69d4a8d36b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code goes here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17226c56-e6cb-4354-b3d7-be2f38345c89",
   "metadata": {},
   "source": [
    "**Note:** Since Twitter API is currently unavailable, we also provide a `timelines.csv` file for you that includes tweets from July 5 to July 12. You can find it in the ZIP file, so please unzip it before using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2f4388-121d-42a0-aa89-f52048da576e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code goes here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660c2bf5-29c6-451e-af54-893849528056",
   "metadata": {},
   "source": [
    "# 2. Build social networks of retweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df9e2ad",
   "metadata": {},
   "source": [
    "Make sure to first load the timelines and users from the files, parse the creation date as datetime and convert the IDs to srings to avoid int overflows. Use `pandas` [`read_csv`](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) to do so. Especcialy look at the keyword arguments `dtype` and `parse_dates`.\n",
    "\n",
    " Since the `referenced_tweets` field contains a list of dictionaries that is stored as a string, we need to parse it first to restore its structure as list of dictionaries to interact with it. For this you can use a combination of pythons [`eval`](https://docs.python.org/3/library/functions.html#eval) function and `pandas` [`apply`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.apply.html) method. This function takes a string and evaluates it as a python expression. For example if you have following String:\n",
    "```\n",
    "some_string_with_a_dict = '{\"first_key\": \"first_value\",\n",
    "                            \"second_key\": \"second_value\",\n",
    "                            \"third_key\": \"third_value\",}'\n",
    "```\n",
    "And use `eval` on it like the following:\n",
    "```\n",
    "usable_dictionary_from_string = eval(some_string_with_a_dict)\n",
    "```\n",
    "You get the dictionary as a python local usable dictonary. Try it out!\n",
    "Now you can convert every `\"referenced_tweets\"` field in the dataset to usable python expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731d605d-3e1c-4237-8442-6fcb73074dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code goes here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a987f77-803b-438a-8e00-d35786dd8ae4",
   "metadata": {},
   "source": [
    "The field `referenced_tweets` currently contains a list where each entry is a tweet object (since we requested the expansion on the field `referenced_tweets.id`.\n",
    "\n",
    "To construct our retweet network, we need to know (a) whether a tweet was a retweet and (b) the ID of the account that posted the tweet that was retweeted. Below we define two functions that help us extract this information from the `referenced_tweets` field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce16548-973a-4d78-90d9-7111e8c9f17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_retweet(entry):\n",
    "    '''Checks whether a tweet is a retweet'''\n",
    "    if entry != entry: # NaN check\n",
    "        return False\n",
    "    for reference in entry:\n",
    "        if reference[\"type\"] == \"retweeted\":\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def get_retweet_author(entry):\n",
    "    '''Returns the author ID of the retweeted tweet'''\n",
    "    if entry != entry: # NaN check\n",
    "        return np.nan\n",
    "    for reference in entry:\n",
    "        if reference[\"type\"] == \"retweeted\":\n",
    "            return reference[\"author_id\"]\n",
    "    return np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2348df86-bb5f-445f-a1e4-b4d1a7e3944d",
   "metadata": {},
   "source": [
    "Apply the functions `check_retweet()` and `get_retweet_author` to the column `referenced_tweets` and create two new columns `retweeted` and `retweet_user_id` containing the relevant information. Again you can use `pandas` [`apply`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.apply.html) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d217d6-6a89-4f08-9beb-847ff6a07805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code goes here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa4746b-96d1-43e8-bac8-7ac92cfb27fa",
   "metadata": {},
   "source": [
    "Filter the tweets in the timelines such that you only retain retweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afadf983-11f3-415c-bc5e-850ec736a079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code goes here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3d33b7-25ff-4907-a765-89da6634710c",
   "metadata": {},
   "source": [
    "Now filter the timelines such that the `retweet_user_id` is one of the user IDs in the user list. Use `pandas` [`isin`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.isin.html) method for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c87fd91-cd68-4efa-9230-88cff7dce374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code goes here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0a5575",
   "metadata": {},
   "source": [
    "Now finaly we can start to create the Graph, start by creating an empty graph with `networkx`. See [here](https://networkx.org/documentation/stable/reference/introduction.html#graph-creation) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a67723-cb41-42be-992e-1c81ecaaea27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code goes here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94116f1c-7aa7-4be9-87a3-69f9d40f4212",
   "metadata": {},
   "source": [
    "To build the retweet network, we have to fill the empty graph object we just created with nodes and edges. For this purpose, we prepare a list of nodes and their attributes and a list of edges.\n",
    "\n",
    "First, construct a list of vertices (nodes) and node attributes containing the user ids,  screen_names, and the **political party label** of the vertices. Remove all users without a party. Each entry of the list has the following form: \n",
    "\n",
    "`(id, {\"username\":username, \"party\":party})`\n",
    "\n",
    "Use the function [`add_nodes_from`](https://networkx.org/documentation/stable/reference/classes/generated/networkx.Graph.add_nodes_from.html) provided by `networkx` to add the nodes to the graph. \n",
    "\n",
    "Then build a list of edges where every edge is a pair of two users that exchanged at least one retweet with each other (regardless of the direction, remove all duplicate entries). Each entry of the list has the following form:  \n",
    "\n",
    "`(author_id_1, author_id_2)`\n",
    "\n",
    "Use the function [`add_edges_from`](https://networkx.org/documentation/stable/reference/classes/generated/networkx.Graph.add_edges_from.html) provided by `networkx` to add the edges to the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfe5c8e-a0e3-48b9-a750-a39dc9db196a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code goes here!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b97ed3-09ef-4cce-a2b7-7d176fe99013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code goes here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d5b887",
   "metadata": {},
   "source": [
    "Visualize the graph - this is very ugly!\n",
    "Use [`draw_networkx`](https://networkx.org/documentation/stable/reference/generated/networkx.drawing.nx_pylab.draw_networkx.html) for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5240f4f3-df85-49b6-8b5c-6907b41c44fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code goes here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f9f1b8-1b55-43b5-bc37-7389716008d5",
   "metadata": {},
   "source": [
    "# 3. Calculate graph assortativity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083ad210-6321-4625-9732-7da2c639da9d",
   "metadata": {},
   "source": [
    "Use the function [`attribute_assortativity_coefficient`](https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.assortativity.attribute_assortativity_coefficient.html) of `networkx` to calculate the assortativity with respect to party labels. How high is the value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eed1868-86cc-4752-b88a-b8a15e6cdfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code goes here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbc8610-98e4-44e3-8774-ccf7d4e54922",
   "metadata": {},
   "source": [
    "To see if the assortativity value fits your expectations, use the function [`draw_networkx`](https://networkx.org/documentation/stable/reference/generated/networkx.drawing.nx_pylab.draw_networkx.html) to plot the network coloring each node according to the political party label of the politician. Does the pattern of colors fit the value of assortativity?\n",
    "\n",
    "Hint 1: use the optional function parameters `nodelist` and `node_color` to pass a list of nodes and a list of corresponding colors to the drawing function.  \n",
    "Hint 2: you can use one of [matplotlibs categorical color maps](https://matplotlib.org/stable/tutorials/colors/colormaps.html) to get a nice series of distinct colors for the parties. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce46f1c3-8297-4773-8689-b0d391b9a282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code goes here!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a80be4e-32e1-476e-8e58-69b9e4b96976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code goes here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399248d1-6cdc-46df-a15e-93c54393ae28",
   "metadata": {},
   "source": [
    "# 4. Permutation tests\n",
    "\n",
    "The above result looks assortative, but how can we test if it could have happened at random and not because of party identity? Here were are going to test it with a permutation test.\n",
    "\n",
    "First, let's run a permutation. Perform the same assortativity calculation as above but permuting the party labels of nodes. You can do this very efficiently by using `pandas` [`sample`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sample.html) function.\n",
    "\n",
    "Also set the party for each node as node attribute by using [`set_node_attribute`](https://networkx.org/documentation/stable/reference/generated/networkx.classes.function.set_node_attributes.html) (be carefull the parties are now permuted)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab7c449-46b0-42a3-be31-0100ac8eb962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code goes here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349eda64-ccf7-4c28-881b-3021fbafad2c",
   "metadata": {},
   "source": [
    "Is the value much closer to zero?\n",
    "Repeat the calculation with 1000 permutations and plot the histogram of the resulting values. Add a line with the value of the assortativity without permutation. Is it far or close to the permuted values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ccb0e0-12bc-4917-acd2-3d10816498dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code goes here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9ec0ff-8744-4738-a95e-5d787b71822a",
   "metadata": {},
   "source": [
    "To be sure, let's calculate a p-value for the null hypothesis that the assortativity is zero and the alternative hypothesis that it is positive (what we expected):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb8e14e-8cad-4b08-9409-cbbeef8231a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code goes here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c4e406-dd0a-4c3a-a47b-9c92fa3686d0",
   "metadata": {},
   "source": [
    "After looking at the above results, do you think it is likely that the assortativity we found in the data was produced by chance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d858242-9686-4a29-8b7e-7b50fdbea324",
   "metadata": {},
   "source": [
    "# 5. Community detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91874d7b-7a87-4b78-bb88-02c4fd3e62cc",
   "metadata": {},
   "source": [
    "Let's test if Twitter communities match political affiliations. Remove nodes with degree zero in the network and run the [Louvain community detection algorithm](https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.community.louvain.louvain_communities.html). Visualize the result coloring nodes by community labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fcc80b-d3b7-47fa-94ef-44d05bc86fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code goes here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4ed190-ff56-4210-a8ce-e366fa418ba4",
   "metadata": {},
   "source": [
    "Run the [`modularity`](https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.community.quality.modularity.html) function with the above community labels. Is it high enough to think that the network has a community structure?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e27434-547c-43b3-8c33-350fb3f6cc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code goes here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeec818d-32f9-4c6f-be91-f475f3127da7",
   "metadata": {},
   "source": [
    "Repeat but using the party labels instead of the communities detected with Louvain. Is it higher or lower? How far is this modularity from the maximal one found with Louvain?\n",
    "\n",
    "For this iterate over the parties and filter a subset of users that is in the given party and in the graph. Add the ids of these partymembers (do not include any duplicates) and repeat this for all parties.\n",
    "\n",
    "Afterwards you can calculate the modularity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71053121-99f9-4754-a69a-b182a0d672c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code goes here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462a4aa1-0b8e-4c01-bb60-aaad998eabac",
   "metadata": {},
   "source": [
    "Finally, to understand which parties are represented in each community, build a data frame for nodes with two columns: one with the party label and another one with the community label. Use the [`groupby()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html) function to print a contingency table. Which party or parties compose each community?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936a63bc-a71d-4228-822c-f8777c6654e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code goes here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495e2320-8af1-4a72-b50e-66ce0efdbce6",
   "metadata": {},
   "source": [
    "# To learn more\n",
    "* How well can you predict the party of a politician from its neighbors in the network? Here you can use the rule of predicting the party as the majority party among its neighbors and evaluate the accuracy of this approach.\n",
    "* What would be the results if we use the network of replies? Do you expect assortativity and modularity to be higher or lower?\n",
    "* If you retrieved data of follower links, you can repeat the above analysis for undirected following relationships. Do you expect a higher or lower assortativity?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ff66a5",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "## Reddit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686ebddb",
   "metadata": {},
   "source": [
    "### Exercise: Data Collection\n",
    "\n",
    "#### Sign up for the Reddit API\n",
    "* In this part of the assignment we will collect data using the Reddit API, and compare the tree structure of political and non-political subreddits.\n",
    "* First, you need to sign up for the Reddit API. For this, follow the steps outlined in [this guide](https://towardsdatascience.com/how-to-use-the-reddit-api-in-python-5e05ddfd1e5c). You will need to create an app on the following [link](https://old.reddit.com/prefs/apps/).\n",
    "* Next, install the [PRAW package](https://praw.readthedocs.io/en/stable/getting_started/quick_start.html), which provides a nice wrapper for the Reddit API.\n",
    "\n",
    "#### Collect the data\n",
    "* Navigate to the following [link](https://www.reddit.com/best/communities/1/) and select 4 political, and 4 non-political subreddits. Ideally, you would want subreddits with around 100-200 thousand members. You want subreddits with enough engagement, but ones which do not typically have a thousand replies to each submission, since the API has a relatively low rate limit.\n",
    "* Extract the top 20 `hottest` submissions from each of your selected subreddits, ignoring `pinned` submissions.\n",
    "* FOr each of the submissions, extract all the comments and replies, and store them, so that you don't need to rerun this step later. Make sure to save the `id`, of the post, the id of its `parent` (the post that it replies to) and the name of the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad5864b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from praw.models import Submission, Comment\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "load_dotenv();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388d3f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code goes here!\n",
    "\n",
    "political_subs = [...]\n",
    "non_political_subs = [...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0a4acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code goes here!\n",
    "\n",
    "# Reddit API communication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55950fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code goes here!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ff719c",
   "metadata": {},
   "source": [
    "### Exercise: Analysis\n",
    "* Create a network/tree for each of the submissions, for this, you may use the [networkx](https://networkx.org/documentation/stable/tutorial.html) package, or create your own classes to store the data.\n",
    "* For each of the trees, calculate the `maximum depth` and `maximum width`. By maximum depth, we mean the number of edges between the root node, and the furthest leaf node (i.e. the reply which is deepest in the comment tree). The maximum width of the tree is the maximum number of comments, replies on one \"level\". On the first \"level\" is the submission itself, on the next one the comments replying directly to the submission, on the third the comments replying to the comments on the first level, and so on.\n",
    "* Also calculate the `number of nodes` for each of the trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da521456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code goes here!\n",
    "\n",
    "# Create a network/tree for each of the submissions using the networkx package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a9475a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code goes here!\n",
    "\n",
    "# OR alternatively your own classes to store the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a6c97d",
   "metadata": {},
   "source": [
    "### Exercise: Comparison and interpretation\n",
    "* Compare the mean number of nodes, mean maximum width, and mean maximum depth of political and non-political subreddits. What differences can you notice?\n",
    "* Can you conduct a statistical test to see if the differences are significant?\n",
    "* Create a scatterplot with the max depth of the tree on the x-axis, and the max width of the tree on the y-axis. Color the dots based on their group (political vs. non-political). Interpret your results.\n",
    "* What are the limitations of this analysis? What would you add, or how would you improve it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04aa5a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0242d78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code goes here!\n",
    "\n",
    "# compare means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c62f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ttest_ind\n",
    "from scipy.stats import ttest_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513fed61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code goes here!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61149f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code goes here!\n",
    "\n",
    "# plot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "46e2835a142a16ae115bce5fddf19f27ce13b17a4ab8ded638c88ab5ce5171d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
